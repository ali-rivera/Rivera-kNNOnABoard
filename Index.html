<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>index</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 70%;
      padding-left: 10%;
      padding-right: 10%;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
        text-align: "center";
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    .center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 25%;
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
      text-align: "center";
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 75%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    table.header{
      border-left: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border-bottom: 0px solid #ddd;
      border-top: 0px solid #ddd;
    }
    tr {
      border-bottom: 1px solid #ddd;
      border-top: 1px solid #ddd;
      border-left: 1px solid #ddd;
      border-right: 1px solid #ddd;
      border: 1px solid #ddd;
    }
    header {
      margin-bottom: 4em;
      text-align: "center";
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h1 id="knn-on-a-board">kNN on a Board</h1>
<table class="header">
  <colgroup>
  <col style="width: 40%" />
  <col style="width: 60%" />
  </colgroup>
  <tbody>
    <td><div style="text-align: center; font-size:25px">An unplugged kNN introductory activity for Data Science Students</div></td>
<td><div style="text-align: center;">
  <img src="kNN-class.jpg" width="25%" alt="An image of 3 students in a data science active learning classroom. They are smiling and holding up a whiteboard with points on it and annotations from the activity they just completed."> <br /><br />
  </div></td>
  </tbody>
  </table>

<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 92%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Summary</strong></td>
<td>This activity allows students to walk through the k Nearest Neighbor
(kNN) machine learning algorithm, a classic first model learned in
machine learning, in an easily interpretable, physical space printed
onto whiteboards. Using dry erase markers, students walk through kNN,
performing the various tasks associated with classifying test points,
tuning the hyperparameter, and creating boundaries in space using the
data on the board. Then, they respond to questions that require them to
communicate their processes and connect their understanding of
overfitting/underfitting to their choice of k value.</td>
</tr>
<tr class="even">
<td><strong>Topics</strong></td>
<td>kNN algorithm, machine learning, overfitting/underfitting,
hyperparameter tuning</td>
</tr>
<tr class="odd">
<td><strong>Audience</strong></td>
<td>Introduction to AI, could be adapted to K-12</td>
</tr>
<tr class="even">
<td><strong>Difficulty</strong></td>
<td>Easy - given students have the assumed prior knowledge described in
the Dependencies section. Otherwise, it is recommended that the
questions be altered to reach students’ level of prior knowledge</td>
</tr>
<tr class="odd">
<td><strong>Strengths</strong></td>
<td>This activity is unplugged and gives students an opportunity to gain
an understanding of the kNN algorithm in a no-code environment. This
activity allows students to explore the algorithm in a familiar,
physical space and gain understanding that can be applied when
implementing the algorithm in a code environment. It also connects
hyperparameter/algorithm tuning to performance visually, rather than
numerically, as it is usually seen when implementing kNN in a computational 
environment. The activity materials are flexible and could be used to
teach multiple facets of kNN and other simple algorithms.</td>
</tr>
<tr class="even">
<td><strong>Weaknesses</strong></td>
<td>As described in the dependencies section, this activity requires
some prior knowledge of foundational machine learning concepts, as well
as some prior knowledge/instruction on the kNN algorithm. We have
included slides that can be used to introduce these concepts in the
Resources section of the materials below. Additionally, creating
boundaries lines at this scale requires approximation, so while creating
the lines helps students understand the boundaries in space, it is not a
perfect representation of how those are made in a ML program.</td>
</tr>
<tr class="odd">
<td><strong>Dependencies</strong></td>
<td>This activity assumes that students have some foundational prior
knowledge of the following terms in the context of data science: <br /> 
  - kNN algorithm <br />     - classification <br />     - training
vs. testing data <br />     - evaluation criteria <br />     -
overfitting vs. underfitting <br /> Materials required for this activity
are: <br />     - printed kNN boards (see comments in instructor notes
about printing) <br />     - dry erase markers or some other erasable
writing utensil <br />     - rulers</td>
</tr>
<tr class="even">
<td><strong>Variants</strong></td>
<td>This activity could be adapted to teach concepts like: <br />    -
<strong>distance metrics</strong> : have students compare classification
outcomes across various distance metrics (Euclidean distance, Manhattan
distance, Minkowski Distance, etc) instead of/in addition to k values.
<br />   - <strong>probability</strong>: have students create the predicted probabilities for the test set for various k values, demonstrating how
increasing/decreasing k affects model fit <br />    - <strong>evaluation
metrics</strong>: have students create a confusion matrix, calculate
accuracy, sensitivity, specificity, etc. for each of their k
values.</td>
</tr>
</tbody>
</table>
<a href="student-activity.html">Student Activity</a> <br />
<a href="instructor-notes.html">Instructor Resources & Notes</a><br />
<a href="kNN_output.pdf">kNN board</a><br />
</ul>
</body>
</html>
